---
title: "DS 705 Final Project"
author: "Jim Ryan"
date: "03/01/2020"
output:
  pdf_document: default
  word_document: default
fontsize: 12pt
---

```{r message=F,warning=F,include=FALSE}
library(data.table)
library(ggplot2)
library(sqldf)
library(caTools)
library(DataExplorer)
library(DescTools)
library(tidyverse)
library(caret)
install.packages("remotes",repos="https://github.com/njtierney/naniar")
library(ResourceSelection)
library(gridExtra)
```
Part 2: Introduction 

For this project I will use logistic regression to predict which applicants are likely to default on their loans. I am going to use the loans50k data set that was given to us as part of this assignment.  This data set contains 50,000 loans in various statuses and various amounts. There are 32 available variables in the data set.  My response variable will be the status variable. I will start by preparing and cleaning the data. 

```{r message=F,warning=F}

LoansDT <- fread("Loans50k.csv")

```
Part 3: Preparing and Cleaning the Data

The data contains different variables collected as either part of the application process or information from when the loan was disbursed. I am trying to predict which variable or variables will allow me to predict whether a loan becomes default or not. I removed all loans that did not have a status of ‘Fully Paid’, ‘Default’ or ‘Charged Off’. Then I copied the response variable – status – into another field called status2. I then updated the status of loans that were Fully Paid to ‘Good’ and the ones that were ‘Default’ or ‘Charged Off’ to ‘Bad’.  

```{r echo=FALSE, message=FALSE,warnings=FALSE,,include=FALSE}
# Delete loans that do not have status 'Charged Off','Default','Fully Paid'
LoansDT <- sqldf(c("Delete from  LoansDT where status not in('Charged Off','Default','Fully Paid')","select * from main.LoansDT")) 
LoansDT$status2 <- LoansDT$status  # create new status column 
#ccreate a second column for amount so I can keep original value when I log the amount value later on
LoansDT$amount2 <- LoansDT$amount
  

# set new status field to 0 for fully paid and 1 for default or charged off
LoansDT <- sqldf(c("update LoansDT set status2 = 'Good' where status2 ='Fully Paid'", "select * from main.LoansDT"))
LoansDT <- sqldf(c("update LoansDT set status2 = 'Bad' where status2 in('Charged Off','Default')","select * from main.LoansDT"))
```

There are 27,074 loans or 78% with a status of ‘Good’ and 7,581 loans or 22 percent with a status of ‘Bad’.  Here is a Histogram of that shows the number of good and bad loans after the status has been updated.    
```{r message=F}
#bar ploit on status
p1 <- ggplot(data=LoansDT, aes(x=status2)) + 
      geom_bar(fill="blue") + 
      xlab("Loans By Status")
p1

#Update good to 1 bad to 0 and make var a factor
LoansDT <- sqldf(c("UPDATE LoansDT SET status2 = 1 where status2 = 'Good'","UPDATE LoansDT SET status2 = 0 where status2 = 'Bad'", "Select * from LoansDT"))

# set status field as factor
LoansDT$status2 <- as.factor(LoansDT$status2)
```
I used sapply with a sum on the NA in the data set to determine which variable had NA in them and how many there were. I found that the only 3 variables that had NA values in them were revolRatio (15 NA), bcOpen (360 NA) and bcRatio (384 NA).Since the missing values were only around 1 percent of the total number for these variables, I imputed them using the mean of all the values for that specific variable.

```{r echo=FALSE, message=FALSE}
# ```{r echo=FALSE, message=FALSE,include=FALSE}

sapply(LoansDT, function(x) sum(is.na(x)))

LoansDT$revolRatio[is.na(LoansDT$revolRatio)] <- mean(LoansDT$revolRatio,na.rm=T)
LoansDT$bcOpen[is.na(LoansDT$bcOpen)] <- mean(LoansDT$bcOpen,na.rm=T)
LoansDT$bcRatio [is.na(LoansDT$bcRatio)] <- mean(LoansDT$bcRatio,na.rm=T)



```
Then I focused my attention on the employment variable. There were 15,268 different values for employment including 1918 where the value is blank. I selected all rows where the value had a count greater than 100.
I updated 1789 values of the employment variable that had the word manager in the to be just ‘Manager’.  There were 934 different variations of the word ‘Teacher’, so I updated them all to be teacher. 

The 'Grade variable was updated from ‘A’ thru ‘G’ to 0 thru 6 and made into a factor.  I update the 'verified' variable by updating those with a value of 'Source Verified' to 'Verified' and made that variable a factor. I did something similar with the home variable. I set the variable to OWN when the value was mortgage so then there were just 2 categorical variables - 'own' and 'rent' made the variable a factor. The variable term only had 2 values (36 months and 60 months) so the variable term2 was updated to a factor. 
```{r message=F,warning=F}

sqldf("Select count(*) from LoansDT where employment like '%manager%' ")
# update all employment rows with word manager in the to Manager
LoansDT <- sqldf(c("update  LoansDT set employment = 'manager' where employment like '%manager%'","Select * from LoansDT"))

# update all employment rows with word manager in the to manager
LoansDT <- sqldf(c("update  LoansDT set employment = 'Teacher' where employment like '%teach%'","Select * from LoansDT"))


#Update grade to numeric (0 thru 6)
 LoansDT <- sqldf(c("UPDATE LoansDT SET grade = CASE grade WHEN 'A' THEN 0 WHEN 'B' THEN 1  WHEN 'C' THEN 2  WHEN 'D' THEN 3 WHEN 'E' THEN 4 WHEN 'F' THEN 5 WHEN 'G' THEN 6 END","Select * from LoansDT"))
# set grade as a factor
 LoansDT$grade <- as.factor(LoansDT$grade)

#Update home to OWN when value is mortgage, set own = 1 and rent = 0 and maake it a factor  
 LoansDT <- sqldf(c("Update LoansDT set home = 'OWN' where home = 'MORTGAGE'","Select * from LoansDT"))
  LoansDT$home <- as.factor(LoansDT$home)

# Update  Source Verified amd Verified to 1 Not verified as 0 make it a factor
  LoansDT <- sqldf(c("UPDATE LoansDT SET verified =  'Verified' where verified =  'Source Verified'","Select * from LoansDT")) 

 LoansDT$verified <- as.factor(LoansDT$verified)
 
# make term column a factor
LoansDT$term <- as.factor(LoansDT$term)

```

I removed the following variables because more than 25 percent of them were 0: pubRec, delinq2yr, and inq6mth. I also dropped the original status variable.

```{r message=F,warning=F}
# remove original status and original term columns
LoansDT$status <- NULL

colnames(LoansDT)
sqldf ("select count (*) from LoansDT where pubRec = 0 ")
sqldf ("select count (*) from LoansDT where delinq2yr = 0 ")
sqldf ("select count (*) from LoansDT where inq6mth = 0 ")

sqldf ("select count(distinct(employment)) from LoansDT") 


 

# following columns are removed as over 25% of values are blank or 0
LoansDT$pubRec <- NULL 
LoansDT$delinq2yr <- NULL 
LoansDT$inq6mth <- NULL


```
Part 4:Exploring and Transforming the Data

Now I delve into some data exploration and transformation. First I do some exploration on the data set as a whole. I create box plots of payment and status to see if there is an observable relationship between payment and status. There is no observable relationship between payment and status for the entire dataset. Then I did the same for openAcc income and loanID The only one that showed an obvious relationship was totalBal. The 'LoanID' variable looks constant no matter what the status. I assume this is just a loan Identifier and will drop it. The other 3 variables I will keep for for part 5.  

```{r  message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
ggplot(aes(x=payment,y=status2),data=LoansDT)+
  geom_boxplot(color='darkblue') + ggtitle("Plot of status by Payment")

ggplot(aes(x=openAcc,y=status2),data=LoansDT)+
  geom_boxplot(color='red') + ggtitle("Plot of status by openAcc")

ggplot(aes(x=income,y=status2),data=LoansDT)+
  geom_boxplot(color='darkred')  + ggtitle("Plot of status by Income")

ggplot(aes(x=loanID,y=status2),data=LoansDT)+
  geom_boxplot(color='green')  + ggtitle("Plot of status by loanid")

LoansDT$loanID <- NULL
```

Then I divided up my modified data frame into good and bad loans based on the status2 variable. Then I made bar graphs of some of the categorical variables divided by good and bad loans to see if there were any obvious relationships. I plotted the  'term', 'verified' and home variable against status2 For the term variable 'bad' loans have a greater percentage of loans with 60 month terms than the 'good' loans.  It appears that there might be some relationship between the 'verified' and term variables and status. I will keep this variables for part 5.

```{r  message=FALSE, warning=FALSE}
library("gridExtra")
loans_bad <- sqldf("Select * from LoansDT where status2 = 0")
loans_good <- sqldf("Select * from LoansDT where status2 = 1")

 
p1 <- ggplot(loans_bad, aes(status2, fill = term)) + facet_wrap(~term)  + geom_bar()
p2 <- ggplot(loans_good, aes(status2, fill = term)) + facet_wrap(~term) + geom_bar()
 

p3 <- ggplot(loans_bad, aes(status2, fill = verified)) + facet_wrap(~verified) + geom_bar()
p4 <- ggplot(loans_good, aes(status2, fill = verified)) + facet_wrap(~verified) + geom_bar()
 

p5 <- ggplot(loans_bad, aes(status2, fill = home)) + facet_wrap(~home) + geom_bar()
p6 <- ggplot(loans_good, aes(status2, fill = home)) + facet_wrap(~home) + geom_bar()

grid.arrange(p1,p2)
grid.arrange(p3,p4)
grid.arrange(p5,p6)

```
I plotted a histogram of all the variables in the LoansDT dataframe to get an overall sense of of the data is normally distributed or not.Due to space limitations, I did not include this plot. Much of  the data looks right skewed.
```{r message=F,include=FALSE}
plot_histogram(LoansDT)
```
I plotted the states against status for both the good and bad loans but noticed any relationship. As expected the more populous states (New York and California) had the higher number of loans both good and bad. I will be dropping this variable.   I am also going to drop the employment field as there are so many different values in there it will slow my regression down as there are over 15,000 distinct values, which is too many for the variable to be significant.
```{r message=F,include=FALSE}

p <- ggplot(loans_bad, aes(status2, fill = state)) + facet_wrap(~state)
p + geom_bar()

p <- ggplot(loans_good, aes(status2, fill = state)) + facet_wrap(~state)
p + geom_bar()

LoansDT$state <- NULL
loans_good$state <- NULL
loans_bad$state <- NULL

LoansDT$employment <- NULL
loans_good$employment <- NULL
loans_bad$employment <- NULL
```
I plotted  variables from each dataframe ('Good' and 'Bad') to determine if the data is normally distributed or not. There is a definite right skew to many of the numeric variables. "rate","Amount", "debt to income ratio" look normally distributed so I will not transform them. To transform the rest of the numeric variables, I created 2 new dataframes for the good and bad loans and with just the numeric fields IO am going to transform.  Then I used the log1p function on the dataframes. I use this function instead of the log function because some of the variables contain zeroes and log has issues with zeroes. I will not transform the following variables as the look normal: "revolRatio" , "accOpen24" and "accOpen24". I plot the data into a histogram again after the log and the data loos much more normally distributed. There are a few that now look left-skewed.  Also after performing it the first time and looking for NA's i found that it created a total to 384 NAs in bcratio between the 2 groups so I am removing that from the dataframes I apply the log1p to.  The debtIncRat and rate fields were also normally distributed so I did not include them in  the group of variables to apply the log1p to. I also did not transform "totalPaid" as we are not supposed to use that values as a predictor. I split the categorical variables into a separate dataframe to join with the logged dataframes later. I plotted several of the variables after the log was applied to the to check the results.
```{r message=F,warning=F}
par(mfrow=c(4,2))
plot_histogram(loans_bad$amount,title="Bad loans amount")
plot_histogram(loans_bad$rate,title="Bad loans rate")
plot_histogram(loans_bad$debtIncRat,title="bad loans Debt to income ratio")
plot_histogram(loans_good$totalIlLim,title="Good loans TotalIlim")

nologvars <- subset(LoansDT,select=c(bcRatio,debtIncRat,rate,totalPaid,amount2))

good_loan_numeric <- loans_good [ c("amount" , "payment" , "income"  , "openAcc"  ,  "revolRatio"  , "totalAcc" , 
                                    "totalBal"    , "totalRevLim" , "accOpen24"   , "avgBal" ,"bcOpen" , "totalLim",  
                                     "totalRevBal"  , "totalBcLim"  , "totalIlLim")]

bad_loan_numeric <- loans_bad[ c("amount" , "payment" , "income"  , "openAcc"     ,  "revolRatio"  , "totalAcc" , 
                                    "totalBal"    , "totalRevLim" , "accOpen24"   , "avgBal" ,"bcOpen" , "totalLim",  
                                     "totalRevBal"  , "totalBcLim"  , "totalIlLim")]

good_loan_discrete <- loans_good[ c("grade", "length" ,"home" ,"verified","reason" ,
                                    "status2","term" )]

bad_loan_discrete <- loans_bad[ c("grade","length" ,"home" ,"verified","reason", 
                                    "status2","term")]
good_loan_log <- log1p(good_loan_numeric)
bad_loan_log <- log1p(bad_loan_numeric)

plot_histogram(good_loan_log$totalIlLim,title="Good loans LOG TotalIlim")
plot_histogram(bad_loan_log$amount,title="Bad loans LOG amount")
```

I checked the dataframes resulting from the log1p function for NA's and found none. I am not showing it to conserve space.
```{r message=F,warning=F,echo=FALSE,include=FALSE}
sapply(good_loan_log, function(x) sum(is.na(x)))
sapply(bad_loan_log, function(x) sum(is.na(x)))
```
Next I will do some density plots with some of the different logged variables  too see if there is a noticeable difference between good and bad loans.  I did density plots of  openacc  and totalLim  for good and bad loans. I didn't see much difference for openacc or totalLim between good or bad loans.


I wasn't able to eliminate many fields in my data exploration but I suspect once I start building my model, that is when I will eliminate more fields.
```{r message=F, include=FALSE}
plot(density(good_loan_log$openAcc))
plot(density(bad_loan_log$openAcc))

plot(density(good_loan_log$totalLim))
plot(density(bad_loan_log$totalLim))

rejoin_df <-cbind(good_loan_discrete , good_loan_log)
rejoin_bad_df <- cbind(bad_loan_discrete , bad_loan_log) 
rejoin_full <- rbind(rejoin_df,rejoin_bad_df)

Loan_regrsn <- cbind(rejoin_full, nologvars)

```
PART 2 - Section 5 - The Logistic Model
For the start of Section 5, I will create 2 datasets from the Loan_regrsn dataframe from Step 4. One dataset will be my training dataset and will contain 80% of the data, the other will be my test dataset and will contain 20% of the data. I end up 6931 in Loan_test and 27724 in Loan_training

```{r}
library(csv)
training_size <- 0.8
set.seed(2112)
training_rows <- sample(seq_len(nrow(Loan_regrsn)), size = floor(training_size * nrow(Loan_regrsn)))
Loan_training <- Loan_regrsn[training_rows, ]
Loan_test <- Loan_regrsn[-training_rows, ]




```

Next I will run the full model and use the summary function to see which variables are significant and I should include in my model. Based upon the coefficients with significant p values I will keep the following values: grade,verified,reason,term,income,revolRatio,totalAcc,totalRevLim,accOpen24,bcOpen,totalRevBal,totalIlLim.  When I run this model the p=values for the following fields are no longer significant:bcOpen,totalRevBal and totalIlLim, so I will drop then and try a third time. A strange thing happened When I ran the third model, T p-values was significant at .013 but it was higher than my second model which had a p-value of .012. The McFadden pseudo R Squared for the third model (.09414) was less than the second model  (.09462) but just barely.Looking at the model only 3 of the 12 values of the reason variable were significant so I decided to drop the reason variable and try it again. The results were much better for the 4th time. The p-value is .0006 with a McFadden Pseudo R2 of .0932. This is the model I will move forward with. To save space, I will only show the p-value for the first and second models. 

```{r}

Loantrain_one <-glm(status2~.,data=Loan_training,family="binomial")
summary(Loantrain_one)

r1 <- PseudoR2(Loantrain_one)
r1[1]


Loantrain_two <- glm(status2~grade+  verified + reason + term + income + revolRatio +totalAcc + totalRevLim + accOpen24 + bcOpen + totalRevBal +totalIlLim  ,data=Loan_training,family="binomial")

summary(Loantrain_two)

r2 <- PseudoR2(Loantrain_two)
r2[1]

Loantrain_three <- glm(status2~grade+  verified  + reason + term + income + revolRatio +totalAcc + totalRevLim + accOpen24 ,data=Loan_training,family="binomial")
summary(Loantrain_three)

r3 <- PseudoR2(Loantrain_three)
r3[1]


Loantrain_Four <- glm(status2~grade +  verified  + term + income + revolRatio +totalAcc + totalRevLim + accOpen24 ,data=Loan_training,family="binomial")




summary(Loantrain_Four)

r4 <- PseudoR2(Loantrain_Four)
r4[1]


```
Below I will use my model created above to predict the status for loans in the test data. After I use my model to predict the status of the loans, I will create a confusion matrix to determine the overall accuracy of the model. The results of the confusion matrix for the model are as follows: I have 5260 loans correctly predicted as good loans and 154 loans correctly predicted as bad loans. So I have a total accuracy percentage of 78%. 98 percent of good loans were correctly predicted as good while only 11 percent of bad loans were correctly predicted as bad. While this model leaves a bit to be desired in predicting the bad loans accurately, it does a much better job accurately predicting the good ones.   
```{r}


test_model <- glm(status2~grade +  verified  + term + income + revolRatio +totalAcc + totalRevLim + accOpen24 ,data=Loan_training,family="binomial")
probabilities <- predict(test_model,newdata=Loan_test, type="response")

 
t <- .5

Bad_or_Good <- ifelse(probabilities>t,1,0)
Bad_or_Good <-  as.factor(Bad_or_Good)

cf <- confusionMatrix(data=Bad_or_Good,reference=Loan_test$status2)
cf$table
cf$overall[1]
 
```
Section 6 - Optimizing the Threshold for Accuracy. 

In this section I am going to vary the threshold from .5 to attempt to correctly predict more bad loans. I will create a confusion matrix for each different threshold and then I will graph them all to show accuracy vs. threshold. As you recall from the previous section my accuracy percentage was 78% but was a much better predictor of 'good' loans than 'bad' loans. I will calculate the accuracy for 5 different thresholds between 0 and 1 to attempt to predict a better percentage of bad loans.  Due to space constraints, I will just show the confusion matrix containing the predictions and the accuracy value for each calculation.

```{r}

# 6 different thresholds
th1 <- .30
th2 <- .45
th3 <- .55
th4 <- .60
th5 <- .75  



Bad_or_Good1 <- ifelse(probabilities>th1,1,0)
Bad_or_Good1 <-  as.factor(Bad_or_Good1)
c1 <- confusionMatrix(data=Bad_or_Good1,reference=Loan_test$status2)
c1$table
c1$overall[1]

  
Bad_or_Good2 <- ifelse(probabilities>th2,1,0)
Bad_or_Good2 <-  as.factor(Bad_or_Good2)
c2 <- confusionMatrix(data=Bad_or_Good2,reference=Loan_test$status2)
c2$table
c2$overall[1]
  

Bad_or_Good3 <- ifelse(probabilities>th3,1,0)
Bad_or_Good3 <-  as.factor(Bad_or_Good3)
c3 <- confusionMatrix(data=Bad_or_Good3,reference=Loan_test$status2)
c3$table
c3$overall[1]
  

Bad_or_Good4 <- ifelse(probabilities>th4,1,0)
Bad_or_Good4 <-  as.factor(Bad_or_Good4)
c4 <- confusionMatrix(data=Bad_or_Good4,reference=Loan_test$status2)
c4$table
c4$overall[1]

Bad_or_Good5 <- ifelse(probabilities>th5,1,0)
Bad_or_Good5 <-  as.factor(Bad_or_Good5)
c5 <- confusionMatrix(data=Bad_or_Good5,reference=Loan_test$status2)
c5$table
c5$overall[1]



```
Given the data in the confusion matrices above, I the accuracy vs the threshold of each of them - including the default (.5). So I did a confusion matrix for 5 thresholds in addition to the default. The values are:.30,.45,.55,60,.75,.85. The highest accuracy level I get is at the .45 and the default threshold.  My accuracy percentages are 78.1% for both the default threshold and 78.09 for the .12 threshold.  The .45 threshold does a little better job at predicting  'good' loans with 5,341 correctly predicted as 'good' compared to 5,260 for the default threshold, but the .5 threshold does a better job at predicting 'bad' loans with 116 'bad' loans predicted correctly. That number is 35 for the .45 threshold. The lower the threshold is ,the better it is at predicting good loans. Unfortunately, the lower the threshold, the worse it is a predicting 'bad' loans. The lowest threshold I used, .3, is 77.5% accurate with 5376 out of 5377 loans correctly predicted as good but it incorrectly predicts all 'bad' loans as good. As the threshold increases from the default level, the overall accuracy drops as the number of 'good' loans predicted correctly drops but the number of 'bad' loans predicted correctly increases. for the .55 threshold, the overall accuracy drops slightly to 77.3%. The number of correctly predicted 'good' loans drops to 4907 but the number of 'bad' loans rises to 469. As the threshold values rises, the number of predicted 'good' loans drops and the number of predicted 'bad' loans rises.   With the threshold set at .65 the number of 'goood' loans is 4907 while the number of bad loans is 409. From the .65 threshold to the .75 threshold the numbers take a dramatic jump. At the .785 threshold there are 3729 loans that are 'good' and 958 'bad' loans. This leaves us with over 2,244 loans that are incorrectly classified as good or bad. The reverse is true as well since as we decrease the threshold from .5 the number of loans classified as good rises and the number of lons classified as bad drops. The accuracy percentage drops as well.
```{r,echo=FALSE}


library(Rmisc)

accur <- cf$overall[1]
dflt_mid <-as.data.frame(accur)
dflt_acc <- cbind(t,dflt_mid)
colnames(dflt_acc)[1] <- "Threshold"
colnames(dflt_acc)[2] <- "Accuracy"


accur1 <- c1$overall[1]
dflt_mid <-as.data.frame(accur1)
dflt_acc_1 <- cbind(th1,dflt_mid)
colnames(dflt_acc_1)[1] <- "Threshold"
colnames(dflt_acc_1)[2] <- "Accuracy"
 


accur2  <- c2$overall[1]
dflt_mid <-as.data.frame(accur2)
dflt_acc_2 <- cbind(th2,dflt_mid)
colnames(dflt_acc_2)[1] <- "Threshold"
colnames(dflt_acc_2)[2] <- "Accuracy"




accur3 <- c3$overall[1]
dflt_mid <-as.data.frame(accur3)
dflt_acc_3 <- cbind(th3,dflt_mid)
colnames(dflt_acc_3)[1] <- "Threshold"
colnames(dflt_acc_3)[2] <- "Accuracy"
 


accur4 <- c4$overall[1]
dflt_mid <-as.data.frame(accur4)
dflt_acc_4 <- cbind(th4,dflt_mid)
colnames(dflt_acc_4)[1] <- "Threshold"
colnames(dflt_acc_4)[2] <- "Accuracy"
 

accur5 <- c5$overall[1]
dflt_mid <-as.data.frame(accur5)
dflt_acc_5 <- cbind(th5,dflt_mid)
colnames(dflt_acc_5)[1] <- "Threshold"
colnames(dflt_acc_5)[2] <- "Accuracy"


acc_thr_df <- rbind(dflt_acc_1,dflt_acc_2,dflt_acc,dflt_acc_3,dflt_acc_4,dflt_acc_5)


p2 <- ggplot(data = acc_thr_df, mapping = aes(x =Accuracy  , y = Threshold)) + 
  geom_point(shape = 18, color = "red", size = 6) +labs(x = " Accuracy", y = "Threshold ",title="Accuracy percentage by Threshold" )  

p2

```
Part 7 - Optimizing the Threshold for Profit
I tnis section I willtake the predictions at the different threshold levels from Part 6 and apply them to my test data. Then I will sum the profit for each threshold and determine what the maximum  profit increase is for the loans I predicted as good. I add the probabilities calculations from section 5 and the test data I created in section 5. Then I sum the total paid minus the amount for my total profit. According to the model, the lowest thresahold  of .3 is where the biggest profit is - 1,418,710. For there the profit for the .45 threshold is  1,374,321. The default threhold profit (.5) is 1,386,706. The profiut for .55 is  1,354,828. The profit keeps decreasing from there. AT .65 the profit is 98,274.20 then for .75 threshold the profit is 966,017.50. The total profit for all the loans in the training set is 1,418,710. The profit for a 'perfect' model that denies all of the truly bad loans is 750778.40.

```{r}

Loan_test_th1 <- cbind(Loan_test,probabilities)


thr1 <- sqldf(c("select sum(totalPaid - amount2) from Loan_test_th1 where probabilities >= .30"))

thr2<-  sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1 where probabilities >= .45 "))

thrdf  <- sqldf(c("select sum(totalPaid - amount2)   from Loan_test_th1 where probabilities >= .50 "))

thr3<-  sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1 where probabilities >= .55 "))

thr4<-  sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1 where probabilities >= .65 "))

thr5<-  sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1 where probabilities >= .75 "))

Prof_no_model <- sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1 where status2 = 1 "))

tot_profit <- sqldf(c("select sum(totalPaid - amount2)  from Loan_test_th1")) 

thr1
thr2
thrdf
thr3
thr4
thr5
Prof_no_model
tot_profit
 







```

